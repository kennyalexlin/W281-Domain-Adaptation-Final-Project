{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 1456, Train labels: 1456\n",
      "Validation images: 415, Validation labels: 415\n",
      "Test images: 210, Test labels: 210\n"
     ]
    }
   ],
   "source": [
    "#### Load in data ####\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from utils import load_images_by_domain, split_images\n",
    "\n",
    "# Define paths\n",
    "img_dir = \"../OfficeCaltechDomainAdaptation/images\"\n",
    "\n",
    "# Load images by domain\n",
    "data_by_domain = load_images_by_domain(\n",
    "    img_dir=img_dir,\n",
    "    target_size=(300, 300),  # Standardized size\n",
    "    method=\"pad\",           # Use padding to maintain aspect ratio\n",
    "    seed=888                # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Split images: Combine amazon and caltech10 into train/val/test\n",
    "train_data, val_data, test_data = split_images(\n",
    "    data_by_domain=data_by_domain,\n",
    "    train_domains=[\"amazon\", \"caltech10\"],  # Combine these for training and validation\n",
    "    test_domains=[],                        # Use part of amazon and caltech10 for testing\n",
    "    train_split=0.7,                        # 60% for training\n",
    "    val_split=0.2,                          # 20% for validation\n",
    "    use_train_for_test=True,                # Use part of train_domains for testing\n",
    "    test_split=0.1,                         # 20% for testing\n",
    "    seed=888                                # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Summary of splits\n",
    "print(f\"Train images: {len(train_data['images'])}, Train labels: {len(train_data['labels'])}\")\n",
    "print(f\"Validation images: {len(val_data['images'])}, Validation labels: {len(val_data['labels'])}\")\n",
    "print(f\"Test images: {len(test_data['images'])}, Test labels: {len(test_data['labels'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GLCM features from 1456 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [00:28<00:00, 51.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GLCM features from 415 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 415/415 [00:06<00:00, 66.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GLCM features from 210 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:03<00:00, 65.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLCM feature extraction and saving completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import extract_glcm_features_split\n",
    "\n",
    "# GLCM parameters\n",
    "glcm_distances = [1, 2, 4, 8]  # Example distances\n",
    "glcm_angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]  # Example angles in radians\n",
    "\n",
    "# Extract GLCM features for each split\n",
    "train_glcm_df = extract_glcm_features_split(train_data, glcm_distances, glcm_angles)\n",
    "val_glcm_df = extract_glcm_features_split(val_data, glcm_distances, glcm_angles)\n",
    "test_glcm_df = extract_glcm_features_split(test_data, glcm_distances, glcm_angles)\n",
    "\n",
    "# Save GLCM features to CSV\n",
    "import os\n",
    "os.makedirs(\"features\", exist_ok=True)\n",
    "train_glcm_df.to_csv(os.path.join(\"features\", \"train_glcm_features.csv\"), index=False)\n",
    "val_glcm_df.to_csv(os.path.join(\"features\", \"val_glcm_features.csv\"), index=False)\n",
    "test_glcm_df.to_csv(os.path.join(\"features\", \"test_glcm_features.csv\"), index=False)\n",
    "\n",
    "print(\"GLCM feature extraction and saving completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL features SVM model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'linear'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.64      0.85      0.73        34\n",
      "        bike       0.58      0.56      0.57        27\n",
      "  calculator       0.42      0.61      0.50        28\n",
      "  headphones       0.67      0.56      0.61        36\n",
      "    keyboard       0.50      0.46      0.48        26\n",
      "      laptop       0.71      0.57      0.63        30\n",
      "     monitor       0.56      0.58      0.57        31\n",
      "       mouse       0.64      0.52      0.57        27\n",
      "         mug       0.53      0.35      0.42        26\n",
      "   projector       0.50      0.59      0.54        27\n",
      "\n",
      "    accuracy                           0.57       292\n",
      "   macro avg       0.57      0.56      0.56       292\n",
      "weighted avg       0.58      0.57      0.57       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the GLCM features from the training data CSV\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target\n",
    "X = train_glcm_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y = train_glcm_df['label']  # Extract target column\n",
    "\n",
    "# Filter numeric columns only\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(numeric_features, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define an SVM pipeline with scaling and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Reduced by Correlation >0.9 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.45      0.68      0.54        34\n",
      "        bike       0.58      0.52      0.55        27\n",
      "  calculator       0.28      0.18      0.22        28\n",
      "  headphones       0.54      0.58      0.56        36\n",
      "    keyboard       0.25      0.15      0.19        26\n",
      "      laptop       0.24      0.30      0.27        30\n",
      "     monitor       0.41      0.42      0.41        31\n",
      "       mouse       0.48      0.37      0.42        27\n",
      "         mug       0.47      0.27      0.34        26\n",
      "   projector       0.41      0.59      0.48        27\n",
      "\n",
      "    accuracy                           0.42       292\n",
      "   macro avg       0.41      0.41      0.40       292\n",
      "weighted avg       0.41      0.42      0.41       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the GLCM features from the training data CSV\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'target'\n",
    "# Separate features and target\n",
    "X = train_glcm_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y = train_glcm_df['label']  # Extract target column\n",
    "\n",
    "# Filter numeric columns only\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Generate the correlation matrix\n",
    "correlation_matrix = numeric_features.corr()\n",
    "\n",
    "# Calculate variance of each feature\n",
    "feature_variance = numeric_features.var()\n",
    "\n",
    "# Initialize a set to keep track of features to drop\n",
    "to_drop = set()\n",
    "\n",
    "# Iterate over the correlation matrix to identify highly correlated features\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:  # Check if correlation is greater than 0.9\n",
    "            # Get feature names\n",
    "            feature_1 = correlation_matrix.columns[i]\n",
    "            feature_2 = correlation_matrix.columns[j]\n",
    "            \n",
    "            # Compare variances and drop the one with lower variance\n",
    "            if feature_variance[feature_1] < feature_variance[feature_2]:\n",
    "                to_drop.add(feature_1)\n",
    "            else:\n",
    "                to_drop.add(feature_2)\n",
    "\n",
    "# Drop the identified features from the dataset\n",
    "reduced_features_df = numeric_features.drop(columns=to_drop)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(reduced_features_df, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define an SVM pipeline with scaling and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotational invariant processing before SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 100, 'svm__gamma': 'scale', 'svm__kernel': 'linear'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.49      0.71      0.58        52\n",
      "        bike       0.49      0.49      0.49        35\n",
      "  calculator       0.51      0.58      0.54        33\n",
      "  headphones       0.49      0.56      0.52        39\n",
      "    keyboard       0.47      0.43      0.45        37\n",
      "      laptop       0.39      0.28      0.33        50\n",
      "     monitor       0.59      0.47      0.53        55\n",
      "       mouse       0.54      0.39      0.45        38\n",
      "         mug       0.33      0.31      0.32        32\n",
      "   projector       0.32      0.36      0.34        44\n",
      "\n",
      "    accuracy                           0.46       415\n",
      "   macro avg       0.46      0.46      0.45       415\n",
      "weighted avg       0.46      0.46      0.46       415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Rotational Invariance Function \n",
    "\n",
    "def make_glcm_rotationally_invariant_no_angle_names(glcm_df, distances, angles, target_column='label'):\n",
    "    # Separate target and features\n",
    "    target = glcm_df[target_column]\n",
    "    features = glcm_df.drop(columns=[target_column])\n",
    "\n",
    "    # Extract numeric features only\n",
    "    numeric_features = features.select_dtypes(include=[np.number])\n",
    "    if numeric_features.empty:\n",
    "        raise ValueError(\"No numeric features found in GLCM DataFrame.\")\n",
    "\n",
    "    # Determine dimensions\n",
    "    N_features = numeric_features.shape[1]\n",
    "    N_dist = len(distances)\n",
    "    N_angles = len(angles)\n",
    "\n",
    "    # N_features must be divisible by N_dist*N_angles\n",
    "    if N_features % (N_dist * N_angles) != 0:\n",
    "        raise ValueError(\"Number of features is not divisible by the number of distances*angles. \"\n",
    "                         \"Cannot reshape features into (dist, angle) groups.\")\n",
    "    \n",
    "    N_base = N_features // (N_dist * N_angles)  # number of base features per distance-angle combo\n",
    "\n",
    "    # Convert to numpy for reshaping\n",
    "    X = numeric_features.values  # shape (n_samples, N_features)\n",
    "\n",
    "    X_reshaped = X.reshape(-1, N_dist, N_angles, N_base)\n",
    "\n",
    "    # Average over angles to get rotational invariance: (n_samples, N_dist, N_base)\n",
    "    X_rot = X_reshaped.mean(axis=2)\n",
    "\n",
    "    # Create column names for rotationally invariant features\n",
    "    # We'll name them as: feature_{base_idx}_dist_{distance}\n",
    "    rot_feature_names = []\n",
    "    for d_idx, d_val in enumerate(distances):\n",
    "        for b_idx in range(N_base):\n",
    "            rot_feature_names.append(f\"feature_{b_idx}_dist_{d_val}_rot_invariant\")\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    rot_features_df = pd.DataFrame(X_rot.reshape(len(X_rot), -1), columns=rot_feature_names, index=glcm_df.index)\n",
    "\n",
    "    # Combine with target\n",
    "    rot_invariant_df = pd.concat([rot_features_df, target], axis=1)\n",
    "\n",
    "    if rot_invariant_df.drop(columns=[target_column]).empty:\n",
    "        raise ValueError(\"Rotationally invariant features are empty after processing.\")\n",
    "    \n",
    "    return rot_invariant_df\n",
    "\n",
    "# Parameters\n",
    "glcm_distances = [1, 2, 4, 8]  # same as extraction\n",
    "glcm_angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "target_column = 'label'  # Adjust if needed\n",
    "\n",
    "# Load Data\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "val_glcm_df = pd.read_csv(\"features/val_glcm_features.csv\")\n",
    "test_glcm_df = pd.read_csv(\"features/test_glcm_features.csv\")\n",
    "\n",
    "# Process Data for Rotational Invariance without angle names\n",
    "rot_train_glcm_df = make_glcm_rotationally_invariant_no_angle_names(train_glcm_df, glcm_distances, glcm_angles, target_column=target_column)\n",
    "rot_val_glcm_df = make_glcm_rotationally_invariant_no_angle_names(val_glcm_df, glcm_distances, glcm_angles, target_column=target_column)\n",
    "rot_test_glcm_df = make_glcm_rotationally_invariant_no_angle_names(test_glcm_df, glcm_distances, glcm_angles, target_column=target_column)\n",
    "\n",
    "# Train/Validation for SVM\n",
    "X_train = rot_train_glcm_df.drop(columns=[target_column])\n",
    "y_train = rot_train_glcm_df[target_column]\n",
    "\n",
    "X_val = rot_val_glcm_df.drop(columns=[target_column])\n",
    "y_val = rot_val_glcm_df[target_column]\n",
    "\n",
    "# Check numeric columns\n",
    "X_train_numeric = X_train.select_dtypes(include=[np.number])\n",
    "X_val_numeric = X_val.select_dtypes(include=[np.number])\n",
    "\n",
    "if X_train_numeric.empty:\n",
    "    raise ValueError(\"X_train_numeric is empty. No numeric features to train on.\")\n",
    "if X_val_numeric.empty:\n",
    "    raise ValueError(\"X_val_numeric is empty. No numeric features for validation.\")\n",
    "\n",
    "# SVM Training with Hyperparameter Tuning\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_numeric, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val_numeric)\n",
    "\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on original features (not rotational invariant ones) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.52      0.65      0.58        34\n",
      "        bike       0.60      0.44      0.51        27\n",
      "  calculator       0.55      0.39      0.46        28\n",
      "  headphones       0.50      0.50      0.50        36\n",
      "    keyboard       0.38      0.38      0.38        26\n",
      "      laptop       0.31      0.37      0.34        30\n",
      "     monitor       0.44      0.39      0.41        31\n",
      "       mouse       0.25      0.30      0.27        27\n",
      "         mug       0.40      0.23      0.29        26\n",
      "   projector       0.38      0.56      0.45        27\n",
      "\n",
      "    accuracy                           0.43       292\n",
      "   macro avg       0.44      0.42      0.42       292\n",
      "weighted avg       0.44      0.43      0.43       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the GLCM features from the training data CSV\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target\n",
    "X = train_glcm_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y = train_glcm_df['label']  # Extract target column\n",
    "\n",
    "# Filter numeric columns only\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(numeric_features, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define an SVM pipeline with scaling, PCA, and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('pca', PCA(n_components=0.95)),  # Keep 95% variance\n",
    "    ('svm', SVC())  # SVM classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected by PCA: 5\n"
     ]
    }
   ],
   "source": [
    "# Access the PCA step from the best pipeline\n",
    "pca_step = grid_search.best_estimator_.named_steps['pca']\n",
    "\n",
    "# Get the number of components chosen by PCA\n",
    "n_components = pca_step.n_components_\n",
    "\n",
    "print(f\"Number of components selected by PCA: {n_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n",
      "Best Model Parameters: {'bootstrap': False, 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 75}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.55      0.65      0.59        34\n",
      "        bike       0.72      0.48      0.58        27\n",
      "  calculator       0.60      0.54      0.57        28\n",
      "  headphones       0.53      0.67      0.59        36\n",
      "    keyboard       0.38      0.31      0.34        26\n",
      "      laptop       0.42      0.50      0.45        30\n",
      "     monitor       0.54      0.45      0.49        31\n",
      "       mouse       0.50      0.44      0.47        27\n",
      "         mug       0.37      0.27      0.31        26\n",
      "   projector       0.39      0.56      0.46        27\n",
      "\n",
      "    accuracy                           0.50       292\n",
      "   macro avg       0.50      0.49      0.49       292\n",
      "weighted avg       0.50      0.50      0.49       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the GLCM features from the training data CSV\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target\n",
    "X = train_glcm_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y = train_glcm_df['label']  # Extract target column\n",
    "\n",
    "# Filter numeric columns only\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(numeric_features, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 75, 100, 200],        # Number of trees\n",
    "    'max_depth': [None, 5, 10, 15, 20],           # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],       # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],         # Minimum samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]             # Whether to use bootstrap samples\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBP testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LBP features from 1456 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [03:12<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LBP features from 415 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 415/415 [00:47<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LBP features from 210 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:23<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBP feature extraction and saving completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import LBP function and grayscale conversion from utils\n",
    "from utils import extract_lbp_features\n",
    "\n",
    "# Define LBP parameters\n",
    "P_values = [4, 8, 16]  # Number of neighbors\n",
    "R_values = [1, 2, 4, 8]    # Radius\n",
    "PR_combinations = list(product(P_values, R_values))  # All (P, R) combinations\n",
    "\n",
    "# Extract LBP features for each split\n",
    "train_lbp_df = extract_lbp_features(train_data, PR_combinations)\n",
    "val_lbp_df = extract_lbp_features(val_data, PR_combinations)\n",
    "test_lbp_df = extract_lbp_features(test_data, PR_combinations)\n",
    "\n",
    "# Save LBP features to CSV in the 'features' subdirectory\n",
    "train_lbp_df.to_csv(os.path.join(\"features\", \"train_lbp_features.csv\"), index=False)\n",
    "val_lbp_df.to_csv(os.path.join(\"features\", \"val_lbp_features.csv\"), index=False)\n",
    "test_lbp_df.to_csv(os.path.join(\"features\", \"test_lbp_features.csv\"), index=False)\n",
    "\n",
    "print(\"LBP feature extraction and saving completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'auto', 'svm__kernel': 'rbf'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.54      0.83      0.66        52\n",
      "        bike       0.69      0.71      0.70        35\n",
      "  calculator       0.49      0.64      0.55        33\n",
      "  headphones       0.59      0.69      0.64        39\n",
      "    keyboard       0.74      0.46      0.57        37\n",
      "      laptop       0.57      0.48      0.52        50\n",
      "     monitor       0.60      0.53      0.56        55\n",
      "       mouse       0.61      0.58      0.59        38\n",
      "         mug       0.44      0.38      0.41        32\n",
      "   projector       0.57      0.45      0.51        44\n",
      "\n",
      "    accuracy                           0.58       415\n",
      "   macro avg       0.59      0.57      0.57       415\n",
      "weighted avg       0.59      0.58      0.57       415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the LBP features from the training data CSV\n",
    "train_lbp_df = pd.read_csv(\"features/train_lbp_features.csv\")\n",
    "val_lbp_df = pd.read_csv(\"features/val_lbp_features.csv\")\n",
    "test_lbp_df = pd.read_csv(\"features/test_lbp_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target for training data\n",
    "X_train = train_lbp_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y_train = train_lbp_df['label']  # Extract target column\n",
    "\n",
    "# For validation set\n",
    "X_val = val_lbp_df.drop(columns=['label'])\n",
    "y_val = val_lbp_df['label']\n",
    "\n",
    "# Filter numeric columns only (if necessary)\n",
    "X_train_numeric = X_train.select_dtypes(include=[np.number])\n",
    "X_val_numeric = X_val.select_dtypes(include=[np.number])\n",
    "\n",
    "# Define an SVM pipeline with scaling and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('svm', SVC())  # SVM classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],      # Penalty parameter of the error term\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
    "    'svm__gamma': ['scale', 'auto']   # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_numeric, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val_numeric)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GABOR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Gabor features from 1456 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [44:10<00:00,  1.82s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Gabor features from 415 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/415 [00:00<?, ?it/s]c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "100%|██████████| 415/415 [12:12<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Gabor features from 210 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/210 [00:00<?, ?it/s]c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "100%|██████████| 210/210 [08:31<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gabor feature extraction and saving completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import extract_gabor_features_split\n",
    "\n",
    "# Define Gabor parameters\n",
    "gabor_frequencies = [0.05, 0.1, 0.2, 0.5]\n",
    "gabor_angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "\n",
    "# Extract Gabor features for each split\n",
    "train_gabor_df = extract_gabor_features_split(train_data, gabor_frequencies, gabor_angles)\n",
    "val_gabor_df = extract_gabor_features_split(val_data, gabor_frequencies, gabor_angles)\n",
    "test_gabor_df = extract_gabor_features_split(test_data, gabor_frequencies, gabor_angles)\n",
    "\n",
    "# Save Gabor features to CSV in the 'features' subdirectory\n",
    "os.makedirs(\"features\", exist_ok=True)\n",
    "train_gabor_df.to_csv(os.path.join(\"features\", \"train_gabor_features.csv\"), index=False)\n",
    "val_gabor_df.to_csv(os.path.join(\"features\", \"val_gabor_features.csv\"), index=False)\n",
    "test_gabor_df.to_csv(os.path.join(\"features\", \"test_gabor_features.csv\"), index=False)\n",
    "\n",
    "print(\"Gabor feature extraction and saving completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.79      0.65      0.72        52\n",
      "        bike       0.60      0.71      0.65        35\n",
      "  calculator       0.54      0.61      0.57        33\n",
      "  headphones       0.55      0.56      0.56        39\n",
      "    keyboard       0.50      0.51      0.51        37\n",
      "      laptop       0.47      0.40      0.43        50\n",
      "     monitor       0.61      0.64      0.62        55\n",
      "       mouse       0.60      0.47      0.53        38\n",
      "         mug       0.45      0.44      0.44        32\n",
      "   projector       0.57      0.70      0.63        44\n",
      "\n",
      "    accuracy                           0.57       415\n",
      "   macro avg       0.57      0.57      0.57       415\n",
      "weighted avg       0.58      0.57      0.57       415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to preprocess features (hardcoded in pipeline for now)\n",
    "def preprocess_features(features):\n",
    "    # Replace infinity and NaN values\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    # Clip values to a reasonable range\n",
    "    features = np.clip(features, -1e6, 1e6)\n",
    "    return features\n",
    "\n",
    "# Load the Gabor features from the training, validation, and test data CSVs\n",
    "train_gabor_df = pd.read_csv(\"features/train_gabor_features.csv\")\n",
    "val_gabor_df = pd.read_csv(\"features/val_gabor_features.csv\")\n",
    "test_gabor_df = pd.read_csv(\"features/test_gabor_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target for training data\n",
    "X_train = train_gabor_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y_train = train_gabor_df['label']  # Extract target column\n",
    "\n",
    "# For validation set\n",
    "X_val = val_gabor_df.drop(columns=['label'])\n",
    "y_val = val_gabor_df['label']\n",
    "\n",
    "# Filter numeric columns only (if necessary)\n",
    "X_train_numeric = X_train.select_dtypes(include=[np.number]).values\n",
    "X_val_numeric = X_val.select_dtypes(include=[np.number]).values\n",
    "\n",
    "# Preprocess training and validation features\n",
    "X_train_numeric = preprocess_features(X_train_numeric)\n",
    "X_val_numeric = preprocess_features(X_val_numeric)\n",
    "\n",
    "# Define an SVM pipeline with scaling and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('svm', SVC())  # SVM classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],      # Penalty parameter of the error term\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
    "    'svm__gamma': ['scale', 'auto']   # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_numeric, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val_numeric)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading Gabor features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading Gabor features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading Gabor features...\n",
      "Number of components capturing 95% variance: 58\n",
      "Validation Accuracy: 0.5951807228915663\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.69      0.77      0.73        52\n",
      "        bike       0.68      0.80      0.74        35\n",
      "  calculator       0.57      0.64      0.60        33\n",
      "  headphones       0.54      0.67      0.60        39\n",
      "    keyboard       0.48      0.57      0.52        37\n",
      "      laptop       0.50      0.50      0.50        50\n",
      "     monitor       0.79      0.62      0.69        55\n",
      "       mouse       0.59      0.42      0.49        38\n",
      "         mug       0.42      0.34      0.38        32\n",
      "   projector       0.61      0.57      0.59        44\n",
      "\n",
      "    accuracy                           0.60       415\n",
      "   macro avg       0.59      0.59      0.58       415\n",
      "weighted avg       0.60      0.60      0.59       415\n",
      "\n",
      "Test Accuracy: 0.6428571428571429\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.61      0.67      0.64        21\n",
      "        bike       0.69      0.78      0.73        23\n",
      "  calculator       0.64      0.67      0.65        21\n",
      "  headphones       0.65      0.76      0.70        17\n",
      "    keyboard       0.62      0.65      0.63        20\n",
      "      laptop       0.72      0.64      0.68        28\n",
      "     monitor       0.62      0.73      0.67        22\n",
      "       mouse       0.73      0.70      0.71        23\n",
      "         mug       0.57      0.24      0.33        17\n",
      "   projector       0.50      0.50      0.50        18\n",
      "\n",
      "    accuracy                           0.64       210\n",
      "   macro avg       0.63      0.63      0.62       210\n",
      "weighted avg       0.64      0.64      0.63       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# File paths for features (only LBP, GLCM, and Gabor)\n",
    "file_paths = {\n",
    "    \"LBP\": {\n",
    "        \"train\": r\"features/train_lbp_features.csv\",\n",
    "        \"val\": r\"features/val_lbp_features.csv\",\n",
    "        \"test\": r\"features/test_lbp_features.csv\",\n",
    "    },\n",
    "    \"GLCM\": {\n",
    "        \"train\": r\"features/train_glcm_features.csv\",\n",
    "        \"val\": r\"features/val_glcm_features.csv\",\n",
    "        \"test\": r\"features/test_glcm_features.csv\",\n",
    "    },\n",
    "    \"Gabor\": {\n",
    "        \"train\": r\"features/train_gabor_features.csv\",\n",
    "        \"val\": r\"features/val_gabor_features.csv\",\n",
    "        \"test\": r\"features/test_gabor_features.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Step 1: Load features from CSV files\n",
    "def load_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"train\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)  # Drop label column\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values  # Use labels from the first feature type\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels  # Combine features horizontally and return labels\n",
    "\n",
    "# Step 2: Clean invalid values in Gabor features\n",
    "def clean_features(features):\n",
    "    \"\"\"\n",
    "    Cleans the input features by handling invalid values:\n",
    "    - Replaces NaN with 0\n",
    "    - Replaces +inf with 1e6 and -inf with -1e6\n",
    "    - Clips extreme values to the range [-1e6, 1e6]\n",
    "    \"\"\"\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    features = np.clip(features, -1e6, 1e6)\n",
    "    return features\n",
    "\n",
    "# Load features\n",
    "train_features, train_labels = load_features({k: v for k, v in file_paths.items()})\n",
    "val_features, val_labels = load_features({k: {\"train\": v[\"val\"]} for k, v in file_paths.items()})\n",
    "test_features, test_labels = load_features({k: {\"train\": v[\"test\"]} for k, v in file_paths.items()})\n",
    "\n",
    "# Preprocess Gabor features specifically (included in all features here)\n",
    "train_features = clean_features(train_features)\n",
    "val_features = clean_features(val_features)\n",
    "test_features = clean_features(test_features)\n",
    "\n",
    "# Step 3: Normalize the training features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "\n",
    "# Step 4: Fit PCA on training features\n",
    "pca = PCA(n_components=0.95, random_state=42)  # Retain 95% of variance\n",
    "train_features_reduced = pca.fit_transform(train_features_normalized)\n",
    "\n",
    "print(f\"Number of components capturing 95% variance: {pca.n_components_}\")\n",
    "\n",
    "# Step 5: Transform validation and test features using the same PCA\n",
    "val_features_normalized = scaler.transform(val_features)  # Use the same scaler\n",
    "val_features_reduced = pca.transform(val_features_normalized)\n",
    "\n",
    "test_features_normalized = scaler.transform(test_features)  # Use the same scaler\n",
    "test_features_reduced = pca.transform(test_features_normalized)\n",
    "\n",
    "# Step 6: Train an SVM model\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_features_reduced, train_labels)\n",
    "\n",
    "# Step 7: Validate the model\n",
    "val_predictions = svm_model.predict(val_features_reduced)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 8: Test the model\n",
    "test_predictions = svm_model.predict(test_features_reduced)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Validation Accuracy: 0.6481927710843374\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.70      0.75      0.72        52\n",
      "        bike       0.69      0.77      0.73        35\n",
      "  calculator       0.55      0.67      0.60        33\n",
      "  headphones       0.57      0.62      0.59        39\n",
      "    keyboard       0.73      0.59      0.66        37\n",
      "      laptop       0.58      0.56      0.57        50\n",
      "     monitor       0.79      0.75      0.77        55\n",
      "       mouse       0.57      0.68      0.62        38\n",
      "         mug       0.53      0.56      0.55        32\n",
      "   projector       0.79      0.50      0.61        44\n",
      "\n",
      "    accuracy                           0.65       415\n",
      "   macro avg       0.65      0.65      0.64       415\n",
      "weighted avg       0.66      0.65      0.65       415\n",
      "\n",
      "Test Accuracy: 0.6285714285714286\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.68      0.71      0.70        21\n",
      "        bike       0.67      0.87      0.75        23\n",
      "  calculator       0.59      0.48      0.53        21\n",
      "  headphones       0.68      0.76      0.72        17\n",
      "    keyboard       0.70      0.70      0.70        20\n",
      "      laptop       0.59      0.71      0.65        28\n",
      "     monitor       0.68      0.68      0.68        22\n",
      "       mouse       0.52      0.57      0.54        23\n",
      "         mug       0.43      0.18      0.25        17\n",
      "   projector       0.64      0.50      0.56        18\n",
      "\n",
      "    accuracy                           0.63       210\n",
      "   macro avg       0.62      0.62      0.61       210\n",
      "weighted avg       0.62      0.63      0.62       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# File paths for features (only LBP, GLCM, and Gabor)\n",
    "file_paths = {\n",
    "    \"LBP\": {\n",
    "        \"train\": r\"features/train_lbp_features.csv\",\n",
    "        \"val\": r\"features/val_lbp_features.csv\",\n",
    "        \"test\": r\"features/test_lbp_features.csv\",\n",
    "    },\n",
    "    \"GLCM\": {\n",
    "        \"train\": r\"features/train_glcm_features.csv\",\n",
    "        \"val\": r\"features/val_glcm_features.csv\",\n",
    "        \"test\": r\"features/test_glcm_features.csv\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 1: Load features from CSV files\n",
    "def load_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"train\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)  # Drop label column\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values  # Use labels from the first feature type\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels  # Combine features horizontally and return labels\n",
    "\n",
    "\n",
    "# Load features\n",
    "train_features, train_labels = load_features({k: v for k, v in file_paths.items()})\n",
    "val_features, val_labels = load_features({k: {\"train\": v[\"val\"]} for k, v in file_paths.items()})\n",
    "test_features, test_labels = load_features({k: {\"train\": v[\"test\"]} for k, v in file_paths.items()})\n",
    "\n",
    "\n",
    "# Step 3: Normalize the training features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "val_features_normalized = scaler.transform(val_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Step 4: Train an SVM model\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_features_normalized, train_labels)\n",
    "\n",
    "# Step 5: Validate the model\n",
    "val_predictions = svm_model.predict(val_features_normalized)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 6: Test the model\n",
    "test_predictions = svm_model.predict(test_features_normalized)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ResNet features...\n",
      "Loading ResNet features...\n",
      "Loading ResNet features...\n",
      "Validation Accuracy: 0.9542168674698795\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.96      0.94      0.95        52\n",
      "        bike       0.97      1.00      0.99        35\n",
      "  calculator       0.97      0.97      0.97        33\n",
      "  headphones       0.95      0.95      0.95        39\n",
      "    keyboard       0.82      0.89      0.86        37\n",
      "      laptop       0.94      0.92      0.93        50\n",
      "     monitor       0.98      0.91      0.94        55\n",
      "       mouse       0.97      1.00      0.99        38\n",
      "         mug       1.00      1.00      1.00        32\n",
      "   projector       0.98      1.00      0.99        44\n",
      "\n",
      "    accuracy                           0.95       415\n",
      "   macro avg       0.95      0.96      0.96       415\n",
      "weighted avg       0.96      0.95      0.95       415\n",
      "\n",
      "Test Accuracy: 0.9714285714285714\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.95      1.00      0.98        21\n",
      "        bike       1.00      1.00      1.00        23\n",
      "  calculator       0.95      0.90      0.93        21\n",
      "  headphones       1.00      0.94      0.97        17\n",
      "    keyboard       0.87      1.00      0.93        20\n",
      "      laptop       1.00      0.93      0.96        28\n",
      "     monitor       1.00      0.95      0.98        22\n",
      "       mouse       0.96      1.00      0.98        23\n",
      "         mug       1.00      1.00      1.00        17\n",
      "   projector       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           0.97       210\n",
      "   macro avg       0.97      0.97      0.97       210\n",
      "weighted avg       0.97      0.97      0.97       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# File paths for features (only LBP, GLCM, and Gabor)\n",
    "file_paths = {\n",
    "    \"ResNet\": {\n",
    "        \"train\": r\"features/train_resnet_features.csv\",\n",
    "        \"val\": r\"features/val_resnet_features.csv\",\n",
    "        \"test\": r\"features/test_resnet_features.csv\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 1: Load features from CSV files\n",
    "def load_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"train\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)  # Drop label column\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values  # Use labels from the first feature type\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels  # Combine features horizontally and return labels\n",
    "\n",
    "\n",
    "# Load features\n",
    "train_features, train_labels = load_features({k: v for k, v in file_paths.items()})\n",
    "val_features, val_labels = load_features({k: {\"train\": v[\"val\"]} for k, v in file_paths.items()})\n",
    "test_features, test_labels = load_features({k: {\"train\": v[\"test\"]} for k, v in file_paths.items()})\n",
    "\n",
    "\n",
    "# Step 3: Normalize the training features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "val_features_normalized = scaler.transform(val_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Step 4: Train an SVM model\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_features_normalized, train_labels)\n",
    "\n",
    "# Step 5: Validate the model\n",
    "val_predictions = svm_model.predict(val_features_normalized)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 6: Test the model\n",
    "test_predictions = svm_model.predict(test_features_normalized)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dslr Test images: 452, Dslr Test labels: 452\n"
     ]
    }
   ],
   "source": [
    "# Extract only test data from the \"dslr\" domain\n",
    "_, _, dslr_webcam_test = split_images(\n",
    "    data_by_domain=data_by_domain,\n",
    "    train_domains=[],            # No training domains needed\n",
    "    test_domains=[\"dslr\", \"webcam\"],       # Use only the \"dslr\" domain for testing\n",
    "    train_split=0.0,             # No training data\n",
    "    val_split=0.0,               # No validation data\n",
    "    test_split=1.0,              # Use all data for testing\n",
    "    use_train_for_test=False,    # Do not mix training domains with testing\n",
    "    seed=888                     # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Print the number of images and labels in the dslr test set\n",
    "print(f\"Dslr Test images: {len(dslr_webcam_test['images'])}, Dslr Test labels: {len(dslr_webcam_test['labels'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'features/dslr_webcam_test_resnet_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mhstack(features), labels\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Load and clean DSLR test features\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m dslr_webcam_test_features, dslr_webcam_test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_dslr_webcam_test_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdslr_webcam_file_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m dslr_webcam_test_features \u001b[38;5;241m=\u001b[39m clean_features(dslr_webcam_test_features)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Normalize and transform DSLR test features using training transformations\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 19\u001b[0m, in \u001b[0;36mload_dslr_webcam_test_features\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method, paths \u001b[38;5;129;01min\u001b[39;00m file_paths\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 19\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features/dslr_webcam_test_resnet_features.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# DSLR & Webcam-specific file paths\n",
    "dslr_webcam_file_paths = {\n",
    "    \"ResNet\": {\"test\": r\"features/dslr_webcam_test_resnet_features.csv\"}\n",
    "}\n",
    "\n",
    "# Function to load DSLR test features\n",
    "def load_dslr_webcam_test_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        df = pd.read_csv(paths[\"test\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels\n",
    "\n",
    "# Load and clean DSLR test features\n",
    "dslr_webcam_test_features, dslr_webcam_test_labels = load_dslr_webcam_test_features(dslr_webcam_file_paths)\n",
    "dslr_webcam_test_features = clean_features(dslr_webcam_test_features)\n",
    "\n",
    "# Normalize and transform DSLR test features using training transformations\n",
    "dslr_webcam_test_features_normalized = scaler.transform(dslr_webcam_test_features)\n",
    "dslr_webcam_test_features_reduced = pca.transform(dslr_webcam_test_features_normalized)\n",
    "\n",
    "# Evaluate the model on DSLR test features\n",
    "dslr_webcam_test_predictions = svm_model.predict(dslr_webcam_test_features_reduced)\n",
    "dslr_webcam_test_accuracy = accuracy_score(dslr_webcam_test_labels, dslr_webcam_test_predictions)\n",
    "\n",
    "print(f\"DSLR & Webcam Test Accuracy: {dslr_webcam_test_accuracy}\")\n",
    "print(\"DSLR & Webcam Test Classification Report:\")\n",
    "print(classification_report(dslr_webcam_test_labels, dslr_webcam_test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
