{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains experiments with various features and components in the modeling process, including:\n",
    "- GLCM Features\n",
    "- LBP Features\n",
    "- GABOR Features\n",
    "- Resnet-101 Features\n",
    "- PCA on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 1456, Train labels: 1456\n",
      "Validation images: 415, Validation labels: 415\n",
      "Test images: 210, Test labels: 210\n"
     ]
    }
   ],
   "source": [
    "#### Load in data ####\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from utils import load_images_by_domain, split_images\n",
    "\n",
    "# Define paths\n",
    "img_dir = \"../OfficeCaltechDomainAdaptation/images\"\n",
    "\n",
    "# Load images by domain\n",
    "data_by_domain = load_images_by_domain(\n",
    "    img_dir=img_dir,\n",
    "    target_size=(300, 300),  # Standardized size\n",
    "    method=\"pad\",           # Use padding to maintain aspect ratio\n",
    "    seed=888                # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Split images: Combine amazon and caltech10 into train/val/test\n",
    "train_data, val_data, test_data = split_images(\n",
    "    data_by_domain=data_by_domain,\n",
    "    train_domains=[\"amazon\", \"caltech10\"],  # Combine these for training and validation\n",
    "    test_domains=[],                        # Use part of amazon and caltech10 for testing\n",
    "    train_split=0.7,                        # 60% for training\n",
    "    val_split=0.2,                          # 20% for validation\n",
    "    use_train_for_test=True,                # Use part of train_domains for testing\n",
    "    test_split=0.1,                         # 20% for testing\n",
    "    seed=888                                # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Summary of splits\n",
    "print(f\"Train images: {len(train_data['images'])}, Train labels: {len(train_data['labels'])}\")\n",
    "print(f\"Validation images: {len(val_data['images'])}, Validation labels: {len(val_data['labels'])}\")\n",
    "print(f\"Test images: {len(test_data['images'])}, Test labels: {len(test_data['labels'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GLCM features from 1456 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [00:28<00:00, 51.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GLCM features from 415 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 415/415 [00:06<00:00, 66.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GLCM features from 210 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:03<00:00, 65.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLCM feature extraction and saving completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import extract_glcm_features_split\n",
    "\n",
    "# GLCM parameters\n",
    "glcm_distances = [1, 2, 4, 8]  # Example distances\n",
    "glcm_angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]  # Example angles in radians\n",
    "\n",
    "# Extract GLCM features for each split\n",
    "train_glcm_df = extract_glcm_features_split(train_data, glcm_distances, glcm_angles)\n",
    "val_glcm_df = extract_glcm_features_split(val_data, glcm_distances, glcm_angles)\n",
    "test_glcm_df = extract_glcm_features_split(test_data, glcm_distances, glcm_angles)\n",
    "\n",
    "# Save GLCM features to CSV\n",
    "import os\n",
    "os.makedirs(\"features\", exist_ok=True)\n",
    "train_glcm_df.to_csv(os.path.join(\"features\", \"train_glcm_features.csv\"), index=False)\n",
    "val_glcm_df.to_csv(os.path.join(\"features\", \"val_glcm_features.csv\"), index=False)\n",
    "test_glcm_df.to_csv(os.path.join(\"features\", \"test_glcm_features.csv\"), index=False)\n",
    "\n",
    "print(\"GLCM feature extraction and saving completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL features SVM model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'linear'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.64      0.85      0.73        34\n",
      "        bike       0.58      0.56      0.57        27\n",
      "  calculator       0.42      0.61      0.50        28\n",
      "  headphones       0.67      0.56      0.61        36\n",
      "    keyboard       0.50      0.46      0.48        26\n",
      "      laptop       0.71      0.57      0.63        30\n",
      "     monitor       0.56      0.58      0.57        31\n",
      "       mouse       0.64      0.52      0.57        27\n",
      "         mug       0.53      0.35      0.42        26\n",
      "   projector       0.50      0.59      0.54        27\n",
      "\n",
      "    accuracy                           0.57       292\n",
      "   macro avg       0.57      0.56      0.56       292\n",
      "weighted avg       0.58      0.57      0.57       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the GLCM features from the training data CSV\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target\n",
    "X = train_glcm_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y = train_glcm_df['label']  # Extract target column\n",
    "\n",
    "# Filter numeric columns only\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(numeric_features, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define an SVM pipeline with scaling and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Reduced by Correlation >0.9 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.45      0.68      0.54        34\n",
      "        bike       0.58      0.52      0.55        27\n",
      "  calculator       0.28      0.18      0.22        28\n",
      "  headphones       0.54      0.58      0.56        36\n",
      "    keyboard       0.25      0.15      0.19        26\n",
      "      laptop       0.24      0.30      0.27        30\n",
      "     monitor       0.41      0.42      0.41        31\n",
      "       mouse       0.48      0.37      0.42        27\n",
      "         mug       0.47      0.27      0.34        26\n",
      "   projector       0.41      0.59      0.48        27\n",
      "\n",
      "    accuracy                           0.42       292\n",
      "   macro avg       0.41      0.41      0.40       292\n",
      "weighted avg       0.41      0.42      0.41       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the GLCM features from the training data CSV\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'target'\n",
    "# Separate features and target\n",
    "X = train_glcm_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y = train_glcm_df['label']  # Extract target column\n",
    "\n",
    "# Filter numeric columns only\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Generate the correlation matrix\n",
    "correlation_matrix = numeric_features.corr()\n",
    "\n",
    "# Calculate variance of each feature\n",
    "feature_variance = numeric_features.var()\n",
    "\n",
    "# Initialize a set to keep track of features to drop\n",
    "to_drop = set()\n",
    "\n",
    "# Iterate over the correlation matrix to identify highly correlated features\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:  # Check if correlation is greater than 0.9\n",
    "            # Get feature names\n",
    "            feature_1 = correlation_matrix.columns[i]\n",
    "            feature_2 = correlation_matrix.columns[j]\n",
    "            \n",
    "            # Compare variances and drop the one with lower variance\n",
    "            if feature_variance[feature_1] < feature_variance[feature_2]:\n",
    "                to_drop.add(feature_1)\n",
    "            else:\n",
    "                to_drop.add(feature_2)\n",
    "\n",
    "# Drop the identified features from the dataset\n",
    "reduced_features_df = numeric_features.drop(columns=to_drop)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(reduced_features_df, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define an SVM pipeline with scaling and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotational invariant processing before SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 100, 'svm__gamma': 'scale', 'svm__kernel': 'linear'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.49      0.71      0.58        52\n",
      "        bike       0.49      0.49      0.49        35\n",
      "  calculator       0.51      0.58      0.54        33\n",
      "  headphones       0.49      0.56      0.52        39\n",
      "    keyboard       0.47      0.43      0.45        37\n",
      "      laptop       0.39      0.28      0.33        50\n",
      "     monitor       0.59      0.47      0.53        55\n",
      "       mouse       0.54      0.39      0.45        38\n",
      "         mug       0.33      0.31      0.32        32\n",
      "   projector       0.32      0.36      0.34        44\n",
      "\n",
      "    accuracy                           0.46       415\n",
      "   macro avg       0.46      0.46      0.45       415\n",
      "weighted avg       0.46      0.46      0.46       415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Rotational Invariance Function \n",
    "\n",
    "def make_glcm_rotationally_invariant_no_angle_names(glcm_df, distances, angles, target_column='label'):\n",
    "    # Separate target and features\n",
    "    target = glcm_df[target_column]\n",
    "    features = glcm_df.drop(columns=[target_column])\n",
    "\n",
    "    # Extract numeric features only\n",
    "    numeric_features = features.select_dtypes(include=[np.number])\n",
    "    if numeric_features.empty:\n",
    "        raise ValueError(\"No numeric features found in GLCM DataFrame.\")\n",
    "\n",
    "    # Determine dimensions\n",
    "    N_features = numeric_features.shape[1]\n",
    "    N_dist = len(distances)\n",
    "    N_angles = len(angles)\n",
    "\n",
    "    # N_features must be divisible by N_dist*N_angles\n",
    "    if N_features % (N_dist * N_angles) != 0:\n",
    "        raise ValueError(\"Number of features is not divisible by the number of distances*angles. \"\n",
    "                         \"Cannot reshape features into (dist, angle) groups.\")\n",
    "    \n",
    "    N_base = N_features // (N_dist * N_angles)  # number of base features per distance-angle combo\n",
    "\n",
    "    # Convert to numpy for reshaping\n",
    "    X = numeric_features.values  # shape (n_samples, N_features)\n",
    "\n",
    "    X_reshaped = X.reshape(-1, N_dist, N_angles, N_base)\n",
    "\n",
    "    # Average over angles to get rotational invariance: (n_samples, N_dist, N_base)\n",
    "    X_rot = X_reshaped.mean(axis=2)\n",
    "\n",
    "    # Create column names for rotationally invariant features\n",
    "    # We'll name them as: feature_{base_idx}_dist_{distance}\n",
    "    rot_feature_names = []\n",
    "    for d_idx, d_val in enumerate(distances):\n",
    "        for b_idx in range(N_base):\n",
    "            rot_feature_names.append(f\"feature_{b_idx}_dist_{d_val}_rot_invariant\")\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    rot_features_df = pd.DataFrame(X_rot.reshape(len(X_rot), -1), columns=rot_feature_names, index=glcm_df.index)\n",
    "\n",
    "    # Combine with target\n",
    "    rot_invariant_df = pd.concat([rot_features_df, target], axis=1)\n",
    "\n",
    "    if rot_invariant_df.drop(columns=[target_column]).empty:\n",
    "        raise ValueError(\"Rotationally invariant features are empty after processing.\")\n",
    "    \n",
    "    return rot_invariant_df\n",
    "\n",
    "# Parameters\n",
    "glcm_distances = [1, 2, 4, 8]  # same as extraction\n",
    "glcm_angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "target_column = 'label'  # Adjust if needed\n",
    "\n",
    "# Load Data\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "val_glcm_df = pd.read_csv(\"features/val_glcm_features.csv\")\n",
    "test_glcm_df = pd.read_csv(\"features/test_glcm_features.csv\")\n",
    "\n",
    "# Process Data for Rotational Invariance without angle names\n",
    "rot_train_glcm_df = make_glcm_rotationally_invariant_no_angle_names(train_glcm_df, glcm_distances, glcm_angles, target_column=target_column)\n",
    "rot_val_glcm_df = make_glcm_rotationally_invariant_no_angle_names(val_glcm_df, glcm_distances, glcm_angles, target_column=target_column)\n",
    "rot_test_glcm_df = make_glcm_rotationally_invariant_no_angle_names(test_glcm_df, glcm_distances, glcm_angles, target_column=target_column)\n",
    "\n",
    "# Train/Validation for SVM\n",
    "X_train = rot_train_glcm_df.drop(columns=[target_column])\n",
    "y_train = rot_train_glcm_df[target_column]\n",
    "\n",
    "X_val = rot_val_glcm_df.drop(columns=[target_column])\n",
    "y_val = rot_val_glcm_df[target_column]\n",
    "\n",
    "# Check numeric columns\n",
    "X_train_numeric = X_train.select_dtypes(include=[np.number])\n",
    "X_val_numeric = X_val.select_dtypes(include=[np.number])\n",
    "\n",
    "if X_train_numeric.empty:\n",
    "    raise ValueError(\"X_train_numeric is empty. No numeric features to train on.\")\n",
    "if X_val_numeric.empty:\n",
    "    raise ValueError(\"X_val_numeric is empty. No numeric features for validation.\")\n",
    "\n",
    "# SVM Training with Hyperparameter Tuning\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_numeric, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val_numeric)\n",
    "\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on original features (not rotational invariant ones) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.52      0.65      0.58        34\n",
      "        bike       0.60      0.44      0.51        27\n",
      "  calculator       0.55      0.39      0.46        28\n",
      "  headphones       0.50      0.50      0.50        36\n",
      "    keyboard       0.38      0.38      0.38        26\n",
      "      laptop       0.31      0.37      0.34        30\n",
      "     monitor       0.44      0.39      0.41        31\n",
      "       mouse       0.25      0.30      0.27        27\n",
      "         mug       0.40      0.23      0.29        26\n",
      "   projector       0.38      0.56      0.45        27\n",
      "\n",
      "    accuracy                           0.43       292\n",
      "   macro avg       0.44      0.42      0.42       292\n",
      "weighted avg       0.44      0.43      0.43       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the GLCM features from the training data CSV\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target\n",
    "X = train_glcm_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y = train_glcm_df['label']  # Extract target column\n",
    "\n",
    "# Filter numeric columns only\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(numeric_features, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define an SVM pipeline with scaling, PCA, and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('pca', PCA(n_components=0.95)),  # Keep 95% variance\n",
    "    ('svm', SVC())  # SVM classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected by PCA: 5\n"
     ]
    }
   ],
   "source": [
    "# Access the PCA step from the best pipeline\n",
    "pca_step = grid_search.best_estimator_.named_steps['pca']\n",
    "\n",
    "# Get the number of components chosen by PCA\n",
    "n_components = pca_step.n_components_\n",
    "\n",
    "print(f\"Number of components selected by PCA: {n_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n",
      "Best Model Parameters: {'bootstrap': False, 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 75}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.55      0.65      0.59        34\n",
      "        bike       0.72      0.48      0.58        27\n",
      "  calculator       0.60      0.54      0.57        28\n",
      "  headphones       0.53      0.67      0.59        36\n",
      "    keyboard       0.38      0.31      0.34        26\n",
      "      laptop       0.42      0.50      0.45        30\n",
      "     monitor       0.54      0.45      0.49        31\n",
      "       mouse       0.50      0.44      0.47        27\n",
      "         mug       0.37      0.27      0.31        26\n",
      "   projector       0.39      0.56      0.46        27\n",
      "\n",
      "    accuracy                           0.50       292\n",
      "   macro avg       0.50      0.49      0.49       292\n",
      "weighted avg       0.50      0.50      0.49       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the GLCM features from the training data CSV\n",
    "train_glcm_df = pd.read_csv(\"features/train_glcm_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target\n",
    "X = train_glcm_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y = train_glcm_df['label']  # Extract target column\n",
    "\n",
    "# Filter numeric columns only\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(numeric_features, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 75, 100, 200],        # Number of trees\n",
    "    'max_depth': [None, 5, 10, 15, 20],           # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],       # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],         # Minimum samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]             # Whether to use bootstrap samples\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBP testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LBP features from 1456 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [03:12<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LBP features from 415 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 415/415 [00:47<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LBP features from 210 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:23<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBP feature extraction and saving completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import LBP function and grayscale conversion from utils\n",
    "from utils import extract_lbp_features\n",
    "\n",
    "# Define LBP parameters\n",
    "P_values = [4, 8, 16]  # Number of neighbors\n",
    "R_values = [1, 2, 4, 8]    # Radius\n",
    "PR_combinations = list(product(P_values, R_values))  # All (P, R) combinations\n",
    "\n",
    "# Extract LBP features for each split\n",
    "train_lbp_df = extract_lbp_features(train_data, PR_combinations)\n",
    "val_lbp_df = extract_lbp_features(val_data, PR_combinations)\n",
    "test_lbp_df = extract_lbp_features(test_data, PR_combinations)\n",
    "\n",
    "# Save LBP features to CSV in the 'features' subdirectory\n",
    "train_lbp_df.to_csv(os.path.join(\"features\", \"train_lbp_features.csv\"), index=False)\n",
    "val_lbp_df.to_csv(os.path.join(\"features\", \"val_lbp_features.csv\"), index=False)\n",
    "test_lbp_df.to_csv(os.path.join(\"features\", \"test_lbp_features.csv\"), index=False)\n",
    "\n",
    "print(\"LBP feature extraction and saving completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'auto', 'svm__kernel': 'rbf'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.54      0.83      0.66        52\n",
      "        bike       0.69      0.71      0.70        35\n",
      "  calculator       0.49      0.64      0.55        33\n",
      "  headphones       0.59      0.69      0.64        39\n",
      "    keyboard       0.74      0.46      0.57        37\n",
      "      laptop       0.57      0.48      0.52        50\n",
      "     monitor       0.60      0.53      0.56        55\n",
      "       mouse       0.61      0.58      0.59        38\n",
      "         mug       0.44      0.38      0.41        32\n",
      "   projector       0.57      0.45      0.51        44\n",
      "\n",
      "    accuracy                           0.58       415\n",
      "   macro avg       0.59      0.57      0.57       415\n",
      "weighted avg       0.59      0.58      0.57       415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the LBP features from the training data CSV\n",
    "train_lbp_df = pd.read_csv(\"features/train_lbp_features.csv\")\n",
    "val_lbp_df = pd.read_csv(\"features/val_lbp_features.csv\")\n",
    "test_lbp_df = pd.read_csv(\"features/test_lbp_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target for training data\n",
    "X_train = train_lbp_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y_train = train_lbp_df['label']  # Extract target column\n",
    "\n",
    "# For validation set\n",
    "X_val = val_lbp_df.drop(columns=['label'])\n",
    "y_val = val_lbp_df['label']\n",
    "\n",
    "# Filter numeric columns only (if necessary)\n",
    "X_train_numeric = X_train.select_dtypes(include=[np.number])\n",
    "X_val_numeric = X_val.select_dtypes(include=[np.number])\n",
    "\n",
    "# Define an SVM pipeline with scaling and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('svm', SVC())  # SVM classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],      # Penalty parameter of the error term\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
    "    'svm__gamma': ['scale', 'auto']   # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_numeric, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val_numeric)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GABOR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Gabor features from 1456 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1456/1456 [44:10<00:00,  1.82s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Gabor features from 415 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/415 [00:00<?, ?it/s]c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "100%|██████████| 415/415 [12:12<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Gabor features from 210 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/210 [00:00<?, ?it/s]c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "100%|██████████| 210/210 [08:31<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gabor feature extraction and saving completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import extract_gabor_features_split\n",
    "\n",
    "# Define Gabor parameters\n",
    "gabor_frequencies = [0.05, 0.1, 0.2, 0.5]\n",
    "gabor_angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "\n",
    "# Extract Gabor features for each split\n",
    "train_gabor_df = extract_gabor_features_split(train_data, gabor_frequencies, gabor_angles)\n",
    "val_gabor_df = extract_gabor_features_split(val_data, gabor_frequencies, gabor_angles)\n",
    "test_gabor_df = extract_gabor_features_split(test_data, gabor_frequencies, gabor_angles)\n",
    "\n",
    "# Save Gabor features to CSV in the 'features' subdirectory\n",
    "os.makedirs(\"features\", exist_ok=True)\n",
    "train_gabor_df.to_csv(os.path.join(\"features\", \"train_gabor_features.csv\"), index=False)\n",
    "val_gabor_df.to_csv(os.path.join(\"features\", \"val_gabor_features.csv\"), index=False)\n",
    "test_gabor_df.to_csv(os.path.join(\"features\", \"test_gabor_features.csv\"), index=False)\n",
    "\n",
    "print(\"Gabor feature extraction and saving completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Model Parameters: {'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.79      0.65      0.72        52\n",
      "        bike       0.60      0.71      0.65        35\n",
      "  calculator       0.54      0.61      0.57        33\n",
      "  headphones       0.55      0.56      0.56        39\n",
      "    keyboard       0.50      0.51      0.51        37\n",
      "      laptop       0.47      0.40      0.43        50\n",
      "     monitor       0.61      0.64      0.62        55\n",
      "       mouse       0.60      0.47      0.53        38\n",
      "         mug       0.45      0.44      0.44        32\n",
      "   projector       0.57      0.70      0.63        44\n",
      "\n",
      "    accuracy                           0.57       415\n",
      "   macro avg       0.57      0.57      0.57       415\n",
      "weighted avg       0.58      0.57      0.57       415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to preprocess features (hardcoded in pipeline for now)\n",
    "def preprocess_features(features):\n",
    "    # Replace infinity and NaN values\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    # Clip values to a reasonable range\n",
    "    features = np.clip(features, -1e6, 1e6)\n",
    "    return features\n",
    "\n",
    "# Load the Gabor features from the training, validation, and test data CSVs\n",
    "train_gabor_df = pd.read_csv(\"features/train_gabor_features.csv\")\n",
    "val_gabor_df = pd.read_csv(\"features/val_gabor_features.csv\")\n",
    "test_gabor_df = pd.read_csv(\"features/test_gabor_features.csv\")\n",
    "\n",
    "# Assume the target variable is labeled as 'label'\n",
    "# Separate features and target for training data\n",
    "X_train = train_gabor_df.drop(columns=['label'])  # Drop the target column to get features\n",
    "y_train = train_gabor_df['label']  # Extract target column\n",
    "\n",
    "# For validation set\n",
    "X_val = val_gabor_df.drop(columns=['label'])\n",
    "y_val = val_gabor_df['label']\n",
    "\n",
    "# Filter numeric columns only (if necessary)\n",
    "X_train_numeric = X_train.select_dtypes(include=[np.number]).values\n",
    "X_val_numeric = X_val.select_dtypes(include=[np.number]).values\n",
    "\n",
    "# Preprocess training and validation features\n",
    "X_train_numeric = preprocess_features(X_train_numeric)\n",
    "X_val_numeric = preprocess_features(X_val_numeric)\n",
    "\n",
    "# Define an SVM pipeline with scaling and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('svm', SVC())  # SVM classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],      # Penalty parameter of the error term\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
    "    'svm__gamma': ['scale', 'auto']   # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_numeric, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val_numeric)\n",
    "\n",
    "# Print classification report for the validation set\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Validation Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading Gabor features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading Gabor features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading Gabor features...\n",
      "Number of components capturing 95% variance: 58\n",
      "Validation Accuracy: 0.5951807228915663\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.69      0.77      0.73        52\n",
      "        bike       0.68      0.80      0.74        35\n",
      "  calculator       0.57      0.64      0.60        33\n",
      "  headphones       0.54      0.67      0.60        39\n",
      "    keyboard       0.48      0.57      0.52        37\n",
      "      laptop       0.50      0.50      0.50        50\n",
      "     monitor       0.79      0.62      0.69        55\n",
      "       mouse       0.59      0.42      0.49        38\n",
      "         mug       0.42      0.34      0.38        32\n",
      "   projector       0.61      0.57      0.59        44\n",
      "\n",
      "    accuracy                           0.60       415\n",
      "   macro avg       0.59      0.59      0.58       415\n",
      "weighted avg       0.60      0.60      0.59       415\n",
      "\n",
      "Test Accuracy: 0.6428571428571429\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.61      0.67      0.64        21\n",
      "        bike       0.69      0.78      0.73        23\n",
      "  calculator       0.64      0.67      0.65        21\n",
      "  headphones       0.65      0.76      0.70        17\n",
      "    keyboard       0.62      0.65      0.63        20\n",
      "      laptop       0.72      0.64      0.68        28\n",
      "     monitor       0.62      0.73      0.67        22\n",
      "       mouse       0.73      0.70      0.71        23\n",
      "         mug       0.57      0.24      0.33        17\n",
      "   projector       0.50      0.50      0.50        18\n",
      "\n",
      "    accuracy                           0.64       210\n",
      "   macro avg       0.63      0.63      0.62       210\n",
      "weighted avg       0.64      0.64      0.63       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# File paths for features (only LBP, GLCM, and Gabor)\n",
    "file_paths = {\n",
    "    \"LBP\": {\n",
    "        \"train\": r\"features/train_lbp_features.csv\",\n",
    "        \"val\": r\"features/val_lbp_features.csv\",\n",
    "        \"test\": r\"features/test_lbp_features.csv\",\n",
    "    },\n",
    "    \"GLCM\": {\n",
    "        \"train\": r\"features/train_glcm_features.csv\",\n",
    "        \"val\": r\"features/val_glcm_features.csv\",\n",
    "        \"test\": r\"features/test_glcm_features.csv\",\n",
    "    },\n",
    "    \"Gabor\": {\n",
    "        \"train\": r\"features/train_gabor_features.csv\",\n",
    "        \"val\": r\"features/val_gabor_features.csv\",\n",
    "        \"test\": r\"features/test_gabor_features.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Step 1: Load features from CSV files\n",
    "def load_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"train\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)  # Drop label column\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values  # Use labels from the first feature type\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels  # Combine features horizontally and return labels\n",
    "\n",
    "# Step 2: Clean invalid values in Gabor features\n",
    "def clean_features(features):\n",
    "    \"\"\"\n",
    "    Cleans the input features by handling invalid values:\n",
    "    - Replaces NaN with 0\n",
    "    - Replaces +inf with 1e6 and -inf with -1e6\n",
    "    - Clips extreme values to the range [-1e6, 1e6]\n",
    "    \"\"\"\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    features = np.clip(features, -1e6, 1e6)\n",
    "    return features\n",
    "\n",
    "# Load features\n",
    "train_features, train_labels = load_features({k: v for k, v in file_paths.items()})\n",
    "val_features, val_labels = load_features({k: {\"train\": v[\"val\"]} for k, v in file_paths.items()})\n",
    "test_features, test_labels = load_features({k: {\"train\": v[\"test\"]} for k, v in file_paths.items()})\n",
    "\n",
    "# Preprocess Gabor features specifically (included in all features here)\n",
    "train_features = clean_features(train_features)\n",
    "val_features = clean_features(val_features)\n",
    "test_features = clean_features(test_features)\n",
    "\n",
    "# Step 3: Normalize the training features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "\n",
    "# Step 4: Fit PCA on training features\n",
    "pca = PCA(n_components=0.95, random_state=42)  # Retain 95% of variance\n",
    "train_features_reduced = pca.fit_transform(train_features_normalized)\n",
    "\n",
    "print(f\"Number of components capturing 95% variance: {pca.n_components_}\")\n",
    "\n",
    "# Step 5: Transform validation and test features using the same PCA\n",
    "val_features_normalized = scaler.transform(val_features)  # Use the same scaler\n",
    "val_features_reduced = pca.transform(val_features_normalized)\n",
    "\n",
    "test_features_normalized = scaler.transform(test_features)  # Use the same scaler\n",
    "test_features_reduced = pca.transform(test_features_normalized)\n",
    "\n",
    "# Step 6: Train an SVM model\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_features_reduced, train_labels)\n",
    "\n",
    "# Step 7: Validate the model\n",
    "val_predictions = svm_model.predict(val_features_reduced)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 8: Test the model\n",
    "test_predictions = svm_model.predict(test_features_reduced)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Validation Accuracy: 0.6481927710843374\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.70      0.75      0.72        52\n",
      "        bike       0.69      0.77      0.73        35\n",
      "  calculator       0.55      0.67      0.60        33\n",
      "  headphones       0.57      0.62      0.59        39\n",
      "    keyboard       0.73      0.59      0.66        37\n",
      "      laptop       0.58      0.56      0.57        50\n",
      "     monitor       0.79      0.75      0.77        55\n",
      "       mouse       0.57      0.68      0.62        38\n",
      "         mug       0.53      0.56      0.55        32\n",
      "   projector       0.79      0.50      0.61        44\n",
      "\n",
      "    accuracy                           0.65       415\n",
      "   macro avg       0.65      0.65      0.64       415\n",
      "weighted avg       0.66      0.65      0.65       415\n",
      "\n",
      "Test Accuracy: 0.6285714285714286\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.68      0.71      0.70        21\n",
      "        bike       0.67      0.87      0.75        23\n",
      "  calculator       0.59      0.48      0.53        21\n",
      "  headphones       0.68      0.76      0.72        17\n",
      "    keyboard       0.70      0.70      0.70        20\n",
      "      laptop       0.59      0.71      0.65        28\n",
      "     monitor       0.68      0.68      0.68        22\n",
      "       mouse       0.52      0.57      0.54        23\n",
      "         mug       0.43      0.18      0.25        17\n",
      "   projector       0.64      0.50      0.56        18\n",
      "\n",
      "    accuracy                           0.63       210\n",
      "   macro avg       0.62      0.62      0.61       210\n",
      "weighted avg       0.62      0.63      0.62       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# File paths for features (only LBP, GLCM, and Gabor)\n",
    "file_paths = {\n",
    "    \"LBP\": {\n",
    "        \"train\": r\"features/train_lbp_features.csv\",\n",
    "        \"val\": r\"features/val_lbp_features.csv\",\n",
    "        \"test\": r\"features/test_lbp_features.csv\",\n",
    "    },\n",
    "    \"GLCM\": {\n",
    "        \"train\": r\"features/train_glcm_features.csv\",\n",
    "        \"val\": r\"features/val_glcm_features.csv\",\n",
    "        \"test\": r\"features/test_glcm_features.csv\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 1: Load features from CSV files\n",
    "def load_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"train\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)  # Drop label column\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values  # Use labels from the first feature type\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels  # Combine features horizontally and return labels\n",
    "\n",
    "\n",
    "# Load features\n",
    "train_features, train_labels = load_features({k: v for k, v in file_paths.items()})\n",
    "val_features, val_labels = load_features({k: {\"train\": v[\"val\"]} for k, v in file_paths.items()})\n",
    "test_features, test_labels = load_features({k: {\"train\": v[\"test\"]} for k, v in file_paths.items()})\n",
    "\n",
    "\n",
    "# Step 3: Normalize the training features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "val_features_normalized = scaler.transform(val_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Step 4: Train an SVM model\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_features_normalized, train_labels)\n",
    "\n",
    "# Step 5: Validate the model\n",
    "val_predictions = svm_model.predict(val_features_normalized)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 6: Test the model\n",
    "test_predictions = svm_model.predict(test_features_normalized)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ResNet features...\n",
      "Loading ResNet features...\n",
      "Loading ResNet features...\n",
      "Validation Accuracy: 0.9542168674698795\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.96      0.94      0.95        52\n",
      "        bike       0.97      1.00      0.99        35\n",
      "  calculator       0.97      0.97      0.97        33\n",
      "  headphones       0.95      0.95      0.95        39\n",
      "    keyboard       0.82      0.89      0.86        37\n",
      "      laptop       0.94      0.92      0.93        50\n",
      "     monitor       0.98      0.91      0.94        55\n",
      "       mouse       0.97      1.00      0.99        38\n",
      "         mug       1.00      1.00      1.00        32\n",
      "   projector       0.98      1.00      0.99        44\n",
      "\n",
      "    accuracy                           0.95       415\n",
      "   macro avg       0.95      0.96      0.96       415\n",
      "weighted avg       0.96      0.95      0.95       415\n",
      "\n",
      "Test Accuracy: 0.9714285714285714\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.95      1.00      0.98        21\n",
      "        bike       1.00      1.00      1.00        23\n",
      "  calculator       0.95      0.90      0.93        21\n",
      "  headphones       1.00      0.94      0.97        17\n",
      "    keyboard       0.87      1.00      0.93        20\n",
      "      laptop       1.00      0.93      0.96        28\n",
      "     monitor       1.00      0.95      0.98        22\n",
      "       mouse       0.96      1.00      0.98        23\n",
      "         mug       1.00      1.00      1.00        17\n",
      "   projector       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           0.97       210\n",
      "   macro avg       0.97      0.97      0.97       210\n",
      "weighted avg       0.97      0.97      0.97       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# File paths for features (only LBP, GLCM, and Gabor)\n",
    "file_paths = {\n",
    "    \"ResNet\": {\n",
    "        \"train\": r\"features/train_resnet_features.csv\",\n",
    "        \"val\": r\"features/val_resnet_features.csv\",\n",
    "        \"test\": r\"features/test_resnet_features.csv\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 1: Load features from CSV files\n",
    "def load_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"train\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)  # Drop label column\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values  # Use labels from the first feature type\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels  # Combine features horizontally and return labels\n",
    "\n",
    "\n",
    "# Load features\n",
    "train_features, train_labels = load_features({k: v for k, v in file_paths.items()})\n",
    "val_features, val_labels = load_features({k: {\"train\": v[\"val\"]} for k, v in file_paths.items()})\n",
    "test_features, test_labels = load_features({k: {\"train\": v[\"test\"]} for k, v in file_paths.items()})\n",
    "\n",
    "\n",
    "# Step 3: Normalize the training features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "val_features_normalized = scaler.transform(val_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Step 4: Train an SVM model\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_features_normalized, train_labels)\n",
    "\n",
    "# Step 5: Validate the model\n",
    "val_predictions = svm_model.predict(val_features_normalized)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 6: Test the model\n",
    "test_predictions = svm_model.predict(test_features_normalized)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading ResNet features...\n",
      "Loading ORB BoVW features...\n",
      "Loading RGB features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading ResNet features...\n",
      "Loading ORB BoVW features...\n",
      "Loading RGB features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading ResNet features...\n",
      "Loading ORB BoVW features...\n",
      "Loading RGB features...\n",
      "Validation Accuracy: 0.9542168674698795\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.98      0.94      0.96        52\n",
      "        bike       0.97      1.00      0.99        35\n",
      "  calculator       1.00      0.94      0.97        33\n",
      "  headphones       0.93      0.95      0.94        39\n",
      "    keyboard       0.83      0.92      0.87        37\n",
      "      laptop       0.94      0.92      0.93        50\n",
      "     monitor       0.96      0.91      0.93        55\n",
      "       mouse       0.97      1.00      0.99        38\n",
      "         mug       1.00      1.00      1.00        32\n",
      "   projector       0.98      1.00      0.99        44\n",
      "\n",
      "    accuracy                           0.95       415\n",
      "   macro avg       0.96      0.96      0.96       415\n",
      "weighted avg       0.96      0.95      0.95       415\n",
      "\n",
      "Test Accuracy: 0.9809523809523809\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       1.00      1.00      1.00        21\n",
      "        bike       1.00      1.00      1.00        23\n",
      "  calculator       1.00      0.95      0.98        21\n",
      "  headphones       0.94      0.94      0.94        17\n",
      "    keyboard       0.95      1.00      0.98        20\n",
      "      laptop       1.00      0.96      0.98        28\n",
      "     monitor       0.96      1.00      0.98        22\n",
      "       mouse       0.96      0.96      0.96        23\n",
      "         mug       1.00      1.00      1.00        17\n",
      "   projector       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           0.98       210\n",
      "   macro avg       0.98      0.98      0.98       210\n",
      "weighted avg       0.98      0.98      0.98       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# File paths for features\n",
    "file_paths = {\n",
    "    \"LBP\": {\n",
    "        \"train\": r\"features/train_lbp_features.csv\",\n",
    "        \"val\": r\"features/val_lbp_features.csv\",\n",
    "        \"test\": r\"features/test_lbp_features.csv\",\n",
    "    },\n",
    "    \"GLCM\": {\n",
    "        \"train\": r\"features/train_glcm_features.csv\",\n",
    "        \"val\": r\"features/val_glcm_features.csv\",\n",
    "        \"test\": r\"features/test_glcm_features.csv\",\n",
    "    },\n",
    "    \"ResNet\": {\n",
    "        \"train\": r\"features/train_resnet_features.csv\",\n",
    "        \"val\": r\"features/val_resnet_features.csv\",\n",
    "        \"test\": r\"features/test_resnet_features.csv\",\n",
    "    },\n",
    "    \"ORB BoVW\": {\n",
    "        \"train\": r\"features/train_orb_bovw_features.csv\",\n",
    "        \"val\": r\"features/val_orb_bovw_features.csv\",\n",
    "        \"test\": r\"features/test_orb_bovw_features.csv\",\n",
    "    },\n",
    "    \"RGB\": {\n",
    "        \"train\": r\"features/train_rgb_features.csv\",\n",
    "        \"val\": r\"features/val_rgb_features.csv\",\n",
    "        \"test\": r\"features/test_rgb_features.csv\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 1: Load features from CSV files\n",
    "def load_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"train\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)  # Drop label column\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values  # Use labels from the first feature type\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels  # Combine features horizontally and return labels\n",
    "\n",
    "# Step 2: Clean invalid values in the features\n",
    "def clean_features(features):\n",
    "    \"\"\"\n",
    "    Cleans the input features by handling invalid values:\n",
    "    - Replaces NaN with 0\n",
    "    - Replaces +inf with 1e6 and -inf with -1e6\n",
    "    - Clips extreme values to the range [-1e6, 1e6]\n",
    "    \"\"\"\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    features = np.clip(features, -1e6, 1e6)\n",
    "    return features\n",
    "\n",
    "# Load and clean features\n",
    "train_features, train_labels = load_features({k: v for k, v in file_paths.items()})\n",
    "train_features = clean_features(train_features)\n",
    "\n",
    "val_features, val_labels = load_features({k: {\"train\": v[\"val\"]} for k, v in file_paths.items()})\n",
    "val_features = clean_features(val_features)\n",
    "\n",
    "test_features, test_labels = load_features({k: {\"train\": v[\"test\"]} for k, v in file_paths.items()})\n",
    "test_features = clean_features(test_features)\n",
    "\n",
    "# Step 3: Normalize the training features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "\n",
    "# Step 4: Transform validation and test features using the same scaler\n",
    "val_features_normalized = scaler.transform(val_features)\n",
    "test_features_normalized = scaler.transform(test_features)\n",
    "\n",
    "# Step 5: Train an SVM model\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_features_normalized, train_labels)\n",
    "\n",
    "# Step 6: Validate the model\n",
    "val_predictions = svm_model.predict(val_features_normalized)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 7: Test the model\n",
    "test_predictions = svm_model.predict(test_features_normalized)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading RGB features...\n",
      "Loading ResNet features...\n",
      "Loading ORB BoVW features...\n",
      "DSLR-Webcam Test Accuracy: 0.09292035398230089\n",
      "DSLR-Webcam Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.00      0.00      0.00        41\n",
      "        bike       0.09      1.00      0.17        42\n",
      "  calculator       0.00      0.00      0.00        43\n",
      "  headphones       0.00      0.00      0.00        40\n",
      "    keyboard       0.00      0.00      0.00        37\n",
      "      laptop       0.00      0.00      0.00        54\n",
      "     monitor       0.00      0.00      0.00        65\n",
      "       mouse       0.00      0.00      0.00        42\n",
      "         mug       0.00      0.00      0.00        35\n",
      "   projector       0.00      0.00      0.00        53\n",
      "\n",
      "    accuracy                           0.09       452\n",
      "   macro avg       0.01      0.10      0.02       452\n",
      "weighted avg       0.01      0.09      0.02       452\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rockhopper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# DSLR-Webcam-specific file paths\n",
    "dslr_webcam_file_paths = {\n",
    "    \"LBP\": {\"test\": r\"features/dslr_webcam_test_lbp_features.csv\"},\n",
    "    \"GLCM\": {\"test\": r\"features/dslr_webcam_test_glcm_features.csv\"},\n",
    "    \"RGB\": {\"test\": r\"features/dslr_webcam_test_rgb_features.csv\"},\n",
    "    \"ResNet\": {\"test\": r\"features/dslr_webcam_test_resnet_features.csv\"},\n",
    "    \"ORB BoVW\": {\"test\": r\"features/dslr_webcam_test_orb_bovw_features.csv\"},\n",
    "}\n",
    "\n",
    "# Function to load DSLR-Webcam test features\n",
    "def load_dslr_webcam_test_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"test\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels\n",
    "\n",
    "# Step 1: Load DSLR-Webcam test features\n",
    "dslr_webcam_test_features, dslr_webcam_test_labels = load_dslr_webcam_test_features(dslr_webcam_file_paths)\n",
    "\n",
    "# Step 2: Clean invalid values in the test features\n",
    "dslr_webcam_test_features = np.nan_to_num(dslr_webcam_test_features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "dslr_webcam_test_features = np.clip(dslr_webcam_test_features, -1e6, 1e6)\n",
    "\n",
    "# Step 3: Normalize the test features using the previously trained scaler\n",
    "dslr_webcam_test_features_normalized = scaler.transform(dslr_webcam_test_features)\n",
    "\n",
    "# Step 4: Predict using the previously trained SVM model\n",
    "dslr_webcam_test_predictions = svm_model.predict(dslr_webcam_test_features_normalized)\n",
    "\n",
    "# Step 5: Evaluate the predictions\n",
    "dslr_webcam_test_accuracy = accuracy_score(dslr_webcam_test_labels, dslr_webcam_test_predictions)\n",
    "print(f\"DSLR-Webcam Test Accuracy: {dslr_webcam_test_accuracy}\")\n",
    "print(\"DSLR-Webcam Test Classification Report:\")\n",
    "print(classification_report(dslr_webcam_test_labels, dslr_webcam_test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading ResNet features...\n",
      "Loading ORB BoVW features...\n",
      "Loading RGB features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading ResNet features...\n",
      "Loading ORB BoVW features...\n",
      "Loading RGB features...\n",
      "Loading LBP features...\n",
      "Loading GLCM features...\n",
      "Loading ResNet features...\n",
      "Loading ORB BoVW features...\n",
      "Loading RGB features...\n",
      "Number of components capturing 95% variance: 411\n",
      "Validation Accuracy: 0.9590361445783132\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.98      0.94      0.96        52\n",
      "        bike       0.97      1.00      0.99        35\n",
      "  calculator       1.00      0.97      0.98        33\n",
      "  headphones       0.90      0.95      0.93        39\n",
      "    keyboard       0.87      0.92      0.89        37\n",
      "      laptop       0.94      0.94      0.94        50\n",
      "     monitor       0.98      0.91      0.94        55\n",
      "       mouse       0.97      1.00      0.99        38\n",
      "         mug       1.00      1.00      1.00        32\n",
      "   projector       0.98      1.00      0.99        44\n",
      "\n",
      "    accuracy                           0.96       415\n",
      "   macro avg       0.96      0.96      0.96       415\n",
      "weighted avg       0.96      0.96      0.96       415\n",
      "\n",
      "Test Accuracy: 0.9809523809523809\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backpack       0.95      1.00      0.98        21\n",
      "        bike       1.00      1.00      1.00        23\n",
      "  calculator       1.00      1.00      1.00        21\n",
      "  headphones       0.94      0.94      0.94        17\n",
      "    keyboard       0.95      1.00      0.98        20\n",
      "      laptop       1.00      0.93      0.96        28\n",
      "     monitor       1.00      1.00      1.00        22\n",
      "       mouse       0.96      0.96      0.96        23\n",
      "         mug       1.00      1.00      1.00        17\n",
      "   projector       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           0.98       210\n",
      "   macro avg       0.98      0.98      0.98       210\n",
      "weighted avg       0.98      0.98      0.98       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# File paths for features\n",
    "file_paths = {\n",
    "    \"LBP\": {\n",
    "        \"train\": r\"features/train_lbp_features.csv\",\n",
    "        \"val\": r\"features/val_lbp_features.csv\",\n",
    "        \"test\": r\"features/test_lbp_features.csv\",\n",
    "    },\n",
    "    \"GLCM\": {\n",
    "        \"train\": r\"features/train_glcm_features.csv\",\n",
    "        \"val\": r\"features/val_glcm_features.csv\",\n",
    "        \"test\": r\"features/test_glcm_features.csv\",\n",
    "    },\n",
    "    \"ResNet\": {\n",
    "        \"train\": r\"features/train_resnet_features.csv\",\n",
    "        \"val\": r\"features/val_resnet_features.csv\",\n",
    "        \"test\": r\"features/test_resnet_features.csv\",\n",
    "    },\n",
    "    \"ORB BoVW\": {\n",
    "        \"train\": r\"features/train_orb_bovw_features.csv\",\n",
    "        \"val\": r\"features/val_orb_bovw_features.csv\",\n",
    "        \"test\": r\"features/test_orb_bovw_features.csv\",\n",
    "    },\n",
    "    \"RGB\": {\n",
    "        \"train\": r\"features/train_rgb_features.csv\",\n",
    "        \"val\": r\"features/val_rgb_features.csv\",\n",
    "        \"test\": r\"features/test_rgb_features.csv\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 1: Load features from CSV files\n",
    "def load_features(file_paths):\n",
    "    features = []\n",
    "    labels = None\n",
    "\n",
    "    for method, paths in file_paths.items():\n",
    "        print(f\"Loading {method} features...\")\n",
    "        df = pd.read_csv(paths[\"train\"])\n",
    "        features.append(df.drop(columns=[\"label\"]).values)  # Drop label column\n",
    "        if labels is None:\n",
    "            labels = df[\"label\"].values  # Use labels from the first feature type\n",
    "        else:\n",
    "            assert np.array_equal(labels, df[\"label\"].values), \"Labels mismatch between features!\"\n",
    "\n",
    "    return np.hstack(features), labels  # Combine features horizontally and return labels\n",
    "\n",
    "# Step 2: Clean invalid values in the features\n",
    "def clean_features(features):\n",
    "    \"\"\"\n",
    "    Cleans the input features by handling invalid values:\n",
    "    - Replaces NaN with 0\n",
    "    - Replaces +inf with 1e6 and -inf with -1e6\n",
    "    - Clips extreme values to the range [-1e6, 1e6]\n",
    "    \"\"\"\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    features = np.clip(features, -1e6, 1e6)\n",
    "    return features\n",
    "\n",
    "# Load and clean features\n",
    "train_features, train_labels = load_features({k: v for k, v in file_paths.items()})\n",
    "train_features = clean_features(train_features)\n",
    "\n",
    "val_features, val_labels = load_features({k: {\"train\": v[\"val\"]} for k, v in file_paths.items()})\n",
    "val_features = clean_features(val_features)\n",
    "\n",
    "test_features, test_labels = load_features({k: {\"train\": v[\"test\"]} for k, v in file_paths.items()})\n",
    "test_features = clean_features(test_features)\n",
    "\n",
    "# Step 3: Normalize the training features\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = scaler.fit_transform(train_features)\n",
    "\n",
    "# Step 4: Apply PCA on training features\n",
    "pca = PCA(n_components=0.95, random_state=42)  # Retain 95% of variance\n",
    "train_features_reduced = pca.fit_transform(train_features_normalized)\n",
    "\n",
    "print(f\"Number of components capturing 95% variance: {pca.n_components_}\")\n",
    "\n",
    "# Step 5: Transform validation and test features using the same scaler and PCA\n",
    "val_features_normalized = scaler.transform(val_features)  # Normalize validation features\n",
    "val_features_reduced = pca.transform(val_features_normalized)  # Apply PCA on validation features\n",
    "\n",
    "test_features_normalized = scaler.transform(test_features)  # Normalize test features\n",
    "test_features_reduced = pca.transform(test_features_normalized)  # Apply PCA on test features\n",
    "\n",
    "# Step 6: Train an SVM model on reduced training features\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_features_reduced, train_labels)\n",
    "\n",
    "# Step 7: Validate the model\n",
    "val_predictions = svm_model.predict(val_features_reduced)\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "\n",
    "# Step 8: Test the model\n",
    "test_predictions = svm_model.predict(test_features_reduced)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
